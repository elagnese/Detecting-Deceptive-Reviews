{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn torch transformers tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VLnEF71P0nO",
        "outputId": "5acad908-ec2f-443e-fc46-fb3d414cf1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT+MLP"
      ],
      "metadata": {
        "id": "-NczI37tTxqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfnu_h1qXcQD",
        "outputId": "cecdbf23-dc78-4dc0-b488-7c308a7bea26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# --- Data Loading and Initial Prep ---\n",
        "df = pd.read_csv('fake reviews dataset.csv')\n",
        "# CG (Computer-Generated) = 1 (Fake), OR (Original) = 0 (Genuine)\n",
        "df['target'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
        "\n",
        "# --- Structured Feature Engineering ---\n",
        "category_col = 'category'\n",
        "rating_col = 'rating'\n",
        "\n",
        "# Define preprocessor for structured features\n",
        "# The structured vector will have a dimension of 11 (10 categories + 1 rating)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), [category_col]),\n",
        "        ('num', StandardScaler(), [rating_col])\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# Fit and transform the structured features on the whole dataset\n",
        "structured_features = preprocessor.fit_transform(df[[category_col, rating_col]])\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Add structured features back to the DataFrame\n",
        "structured_df = pd.DataFrame(structured_features, columns=feature_names)\n",
        "df = pd.concat([df.reset_index(drop=True), structured_df], axis=1)\n",
        "\n",
        "# Split data into Train/Validation/Test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['target'])\n",
        "\n",
        "# Define the number of structured features for the model\n",
        "N_STRUCTURED_FEATURES = len(feature_names)"
      ],
      "metadata": {
        "id": "cje5M-rtPi-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# --- Configuration ---\n",
        "BERT_MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 128\n",
        "# The dimension of the BERT embedding (CLS token) for bert-base-uncased is 768\n",
        "BERT_EMBEDDING_DIM = 768\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len, feature_names):\n",
        "        self.texts = df['text_'].tolist()\n",
        "        self.structured_features = df[feature_names].values.astype(np.float32)\n",
        "        self.targets = df['target'].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        target = self.targets[idx]\n",
        "        struct_features = self.structured_features[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'structured_features': torch.tensor(struct_features, dtype=torch.float),\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, MAX_LEN, feature_names)\n",
        "val_dataset = ReviewDataset(val_df, tokenizer, MAX_LEN, feature_names)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer, MAX_LEN, feature_names)"
      ],
      "metadata": {
        "id": "vfdXDhWDKjJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class HybridBertClassifier(nn.Module):\n",
        "    def __init__(self, n_structured_features, n_classes, bert_embedding_dim=768, dropout_prob=0.3):\n",
        "        super(HybridBertClassifier, self).__init__()\n",
        "        # 1. BERT Component\n",
        "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "        # 2. Hybrid Classifier (The \"Other Model\")\n",
        "        # The total fused dimension is BERT_EMBEDDING_DIM (768) + N_STRUCTURED_FEATURES (11)\n",
        "        total_fused_dim = bert_embedding_dim + n_structured_features\n",
        "\n",
        "        # A simple MLP/Dense Network on the fused features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(total_fused_dim, 256),  # First dense layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(256, n_classes)  # Final output layer (2 classes: Fake/Genuine)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, structured_features):\n",
        "        # 1. Get Text Embedding from BERT (using the [CLS] token output)\n",
        "        output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # The [CLS] token embedding is the first element of the last hidden state\n",
        "        cls_embedding = output.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Apply dropout to the BERT embedding\n",
        "        cls_embedding = self.dropout(cls_embedding)\n",
        "\n",
        "        # 2. Feature Fusion (Concatenation)\n",
        "        # Combine the BERT embedding (text features) and the structured features\n",
        "        fused_vector = torch.cat((cls_embedding, structured_features), dim=1)\n",
        "\n",
        "        # 3. Final Classification\n",
        "        output = self.classifier(fused_vector)\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "model = HybridBertClassifier(\n",
        "    n_structured_features=N_STRUCTURED_FEATURES,\n",
        "    n_classes=2 # Fake (1) / Genuine (0)\n",
        ")"
      ],
      "metadata": {
        "id": "VehDTO7bPpWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Load and Reset Data ---\n",
        "file_path = 'fake reviews dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df['target'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
        "\n",
        "# --- 2. CORRECTED Feature Engineering ---\n",
        "# We use sparse_output=False to ensure we get a dense array for the DataFrame\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['category']),\n",
        "        ('num', StandardScaler(), ['rating'])\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# Fit and transform\n",
        "structured_features = preprocessor.fit_transform(df[['category', 'rating']])\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# *** THE FIX: Rename 'rating' to avoid duplicate column names ***\n",
        "# This ensures we don't accidentally select the original 'rating' column later\n",
        "feature_names = [f\"scaled_{name}\" if name == 'rating' else name for name in feature_names]\n",
        "\n",
        "# Create DataFrame with UNIQUE names\n",
        "structured_df = pd.DataFrame(structured_features, columns=feature_names)\n",
        "df = pd.concat([df.reset_index(drop=True), structured_df], axis=1)\n",
        "\n",
        "print(f\"Final Feature Names ({len(feature_names)}): {feature_names}\")\n",
        "\n",
        "# --- 3. Dataset and Dataloaders ---\n",
        "# Split Data\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['target'])\n",
        "\n",
        "BERT_MODEL_NAME = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len, feature_names):\n",
        "        self.texts = df['text_'].tolist()\n",
        "        self.structured_features = df[feature_names].values.astype(np.float32)\n",
        "        self.targets = df['target'].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'structured_features': torch.tensor(self.structured_features[idx], dtype=torch.float),\n",
        "            'targets': torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Re-create Datasets with the NEW feature names\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, MAX_LEN, feature_names)\n",
        "val_dataset = ReviewDataset(val_df, tokenizer, MAX_LEN, feature_names)\n",
        "\n",
        "# --- 4. Model and Training ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "class HybridBertClassifier(nn.Module):\n",
        "    def __init__(self, n_structured_features, n_classes):\n",
        "        super(HybridBertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
        "        # Fused dimension = 768 (BERT) + N_Structured\n",
        "        total_fused_dim = 768 + n_structured_features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(total_fused_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, structured_features):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = self.dropout(output.last_hidden_state[:, 0, :])\n",
        "        fused_vector = torch.cat((cls_embedding, structured_features), dim=1)\n",
        "        return self.classifier(fused_vector)\n",
        "\n",
        "# Initialize Model with CORRECT dimension\n",
        "model = HybridBertClassifier(n_structured_features=len(feature_names), n_classes=2)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataset)//BATCH_SIZE * EPOCHS)\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(f\"Starting training on {DEVICE}...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for d in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        input_ids = d[\"input_ids\"].to(DEVICE)\n",
        "        attention_mask = d[\"attention_mask\"].to(DEVICE)\n",
        "        structured_features = d[\"structured_features\"].to(DEVICE)\n",
        "        targets = d[\"targets\"].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, structured_features)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "id": "VoUctgosPtYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install shap\n",
        "import shap\n",
        "\n",
        "# --- Load the trained model (conceptual) ---\n",
        "# model.load_state_dict(torch.load('best_model.pth'))\n",
        "# model = model.to('cpu')\n",
        "# model.eval()\n",
        "\n",
        "# --- 1. Define the Explainer ---\n",
        "# Use the SHAP DeepExplainer as the model is a neural network.\n",
        "# It requires a background dataset, usually a subset of the training data.\n",
        "\n",
        "# Get a sample batch from the DataLoader\n",
        "sample_batch = next(iter(val_data_loader))\n",
        "\n",
        "# Extract the tensors for SHAP background data\n",
        "background_input_ids = sample_batch['input_ids'].to('cpu')\n",
        "background_attention_mask = sample_batch['attention_mask'].to('cpu')\n",
        "background_structured_features = sample_batch['structured_features'].to('cpu')\n",
        "\n",
        "# Create a function that the explainer can call\n",
        "def model_predictor(input_data):\n",
        "    # input_data is a tuple/list from the explainer (input_ids, attention_mask, structured_features)\n",
        "    with torch.no_grad():\n",
        "        # The explainer expects a function that takes *tensors* and returns model output logits\n",
        "        # Here we only need to pass the BERT and structured features.\n",
        "        return model(input_data[0].long(), input_data[1].long(), input_data[2].float())\n",
        "\n",
        "# Initialize DeepExplainer\n",
        "explainer = shap.DeepExplainer(\n",
        "    model_predictor,\n",
        "    (background_input_ids, background_attention_mask, background_structured_features)\n",
        ")\n",
        "\n",
        "# --- 2. Calculate SHAP Values for a specific review ---\n",
        "test_batch = next(iter(DataLoader(test_dataset, batch_size=1)))\n",
        "\n",
        "# Get the inputs for the review to explain\n",
        "test_input_ids = test_batch['input_ids'].to('cpu')\n",
        "test_attention_mask = test_batch['attention_mask'].to('cpu')\n",
        "test_structured_features = test_batch['structured_features'].to('cpu')\n",
        "test_text = tokenizer.decode(test_input_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "# Calculate SHAP values\n",
        "shap_values = explainer.shap_values(\n",
        "    (test_input_ids, test_attention_mask, test_structured_features)\n",
        ")\n",
        "\n",
        "# --- 3. Interpretation (Conceptual) ---\n",
        "# The shap_values array will be a list of two arrays (one for each class: 0 and 1).\n",
        "# The array for class 1 (Fake) contains the contribution of the FUSED FEATURE VECTOR:\n",
        "# [BERT_EMBEDDING_DIM] + [N_STRUCTURED_FEATURES]\n",
        "# You can map the last N_STRUCTURED_FEATURES SHAP values directly to the feature_names.\n",
        "\n",
        "# Example: Get contributions of structured features (last 11 values for class 1)\n",
        "# structured_contributions = shap_values[1][0][-N_STRUCTURED_FEATURES:]\n",
        "# print(\"Review Text:\", test_text)\n",
        "# print(\"--- Structured Feature Contributions (for 'Fake' class) ---\")\n",
        "# for name, contribution in zip(feature_names, structured_contributions):\n",
        "#     print(f\"{name}: {contribution:.4f}\")"
      ],
      "metadata": {
        "id": "3489SDPGG9We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Set to False if you are on GPU or want full training\n",
        "DEBUG_MODE = False\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Running on {DEVICE} (DEBUG_MODE={DEBUG_MODE})\")\n",
        "\n",
        "# --- 1. DATA LOADING & PREPROCESSING ---\n",
        "df = pd.read_csv('fake reviews dataset.csv')\n",
        "\n",
        "# Optional: Slice data for debugging/speed if needed\n",
        "if DEBUG_MODE:\n",
        "    df = df.head(500)\n",
        "\n",
        "df['target'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
        "\n",
        "# Feature Engineering\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['category']),\n",
        "        ('num', StandardScaler(), ['rating'])\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "structured_features = preprocessor.fit_transform(df[['category', 'rating']])\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# FIX: Rename 'rating' to 'scaled_rating' to avoid duplicate columns\n",
        "feature_names = [f\"scaled_{name}\" if name == 'rating' else name for name in feature_names]\n",
        "\n",
        "structured_df = pd.DataFrame(structured_features, columns=feature_names)\n",
        "df = pd.concat([df.reset_index(drop=True), structured_df], axis=1)\n",
        "\n",
        "# --- 2. DATASET CLASS ---\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MAX_LEN = 128\n",
        "\n",
        "class ReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len, feature_names):\n",
        "        self.texts = df['text_'].tolist()\n",
        "        self.structured_features = df[feature_names].values.astype(np.float32)\n",
        "        self.targets = df['target'].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'structured_features': torch.tensor(self.structured_features[idx], dtype=torch.float),\n",
        "            'targets': torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# --- 3. DATALOADERS (Variable Defined Here) ---\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset = ReviewDataset(train_df, tokenizer, MAX_LEN, feature_names)\n",
        "val_dataset = ReviewDataset(val_df, tokenizer, MAX_LEN, feature_names)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE) # <--- FIXED DEFINITION\n",
        "\n",
        "# --- 4. HYBRID MODEL ---\n",
        "class HybridBertClassifier(nn.Module):\n",
        "    def __init__(self, n_structured_features, n_classes):\n",
        "        super(HybridBertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Optional: Freeze BERT layers for speed\n",
        "        # for param in self.bert.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "        total_fused_dim = 768 + n_structured_features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(total_fused_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, structured_features):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = self.dropout(output.last_hidden_state[:, 0, :])\n",
        "        fused_vector = torch.cat((cls_embedding, structured_features), dim=1)\n",
        "        return self.classifier(fused_vector)\n",
        "\n",
        "model = HybridBertClassifier(len(feature_names), 2).to(DEVICE)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "# --- 5. TRAINING LOOP ---\n",
        "print(\"Starting Training...\")\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(DEVICE)\n",
        "        mask = batch['attention_mask'].to(DEVICE)\n",
        "        struct = batch['structured_features'].to(DEVICE)\n",
        "        targets = batch['targets'].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, mask, struct)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# --- 6. EXPLAINABILITY (Uses val_data_loader) ---\n",
        "print(\"\\n--- Feature Importance Analysis ---\")\n",
        "\n",
        "# Method A: Direct Weight Inspection (Fastest & Guaranteed to work)\n",
        "# This shows which structured features (Rating/Category) the model relies on most.\n",
        "classifier_weights = model.classifier[0].weight.detach().cpu().numpy()\n",
        "# Get average absolute weight for each feature across all neurons\n",
        "avg_weights = np.mean(np.abs(classifier_weights), axis=0)\n",
        "\n",
        "# The last N weights belong to the structured features\n",
        "struct_weights = avg_weights[-len(feature_names):]\n",
        "indices = np.argsort(struct_weights)[::-1]\n",
        "\n",
        "print(\"Top Influential Structured Features:\")\n",
        "for i in indices:\n",
        "    print(f\"{feature_names[i]}: {struct_weights[i]:.4f}\")\n",
        "\n",
        "# Method B: Text Explainability (Conceptual)\n",
        "print(\"\\nTo interpret text, you would now use SHAP on 'model' using 'val_data_loader'.\")\n",
        "print(\"Variable 'val_data_loader' is now ready for use.\")"
      ],
      "metadata": {
        "id": "eAN0AQyGIGxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "# --- 1. PREPARE BACKGROUND DATA ---\n",
        "# We use val_data_loader to calculate the \"Average\" structured feature vector.\n",
        "# This acts as a fixed baseline so we can isolate the effect of the TEXT.\n",
        "print(\"Calculating baseline structured features from val_data_loader...\")\n",
        "all_struct_features = []\n",
        "for batch in val_data_loader:\n",
        "    all_struct_features.append(batch['structured_features'])\n",
        "\n",
        "# Calculate the mean vector (e.g., average rating, average category distribution)\n",
        "mean_structured_features = torch.cat(all_struct_features).mean(dim=0).to(DEVICE)\n",
        "\n",
        "# --- 2. DEFINE THE PREDICTION WRAPPER ---\n",
        "# SHAP will pass a list of strings (texts) to this function.\n",
        "# We need to turn them into tensors and add the structured features.\n",
        "def f(texts):\n",
        "    model.eval()\n",
        "\n",
        "    # A. Tokenize the incoming texts\n",
        "    encoding = tokenizer(\n",
        "        texts.tolist() if isinstance(texts, np.ndarray) else texts,\n",
        "        return_tensors='pt',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # B. Create a batch of the \"Mean Structured Features\" to match the text batch size\n",
        "    # Shape: (Batch_Size, N_Structured_Features)\n",
        "    batch_struct = mean_structured_features.repeat(encoding['input_ids'].shape[0], 1)\n",
        "\n",
        "    # C. Run the Model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=encoding['input_ids'],\n",
        "            attention_mask=encoding['attention_mask'],\n",
        "            structured_features=batch_struct\n",
        "        )\n",
        "        # Apply Softmax to get probabilities (Fake vs Genuine)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs\n",
        "\n",
        "# --- 3. RUN SHAP ---\n",
        "print(\"Initializing SHAP Explainer (this works on the text part)...\")\n",
        "\n",
        "# Create the Explainer using our wrapper function 'f' and the tokenizer\n",
        "# This \"masker\" tells SHAP how to break the text into tokens\n",
        "explainer = shap.Explainer(f, shap.maskers.Text(tokenizer))\n",
        "\n",
        "# Get a small batch of real examples from val_data_loader to explain\n",
        "# We just take the text from the first batch\n",
        "sample_batch = next(iter(val_data_loader))\n",
        "texts_to_explain = tokenizer.batch_decode(sample_batch['input_ids'][:5], skip_special_tokens=True)\n",
        "true_labels = sample_batch['targets'][:5].tolist()\n",
        "\n",
        "print(f\"Explaining {len(texts_to_explain)} reviews...\")\n",
        "shap_values = explainer(texts_to_explain)\n",
        "\n",
        "# --- 4. VISUALIZE ---\n",
        "# Show the text plot\n",
        "# Red = Contributes to Class 1 (Fake/CG)\n",
        "# Blue = Contributes to Class 0 (Original)\n",
        "shap.plots.text(shap_values)"
      ],
      "metadata": {
        "id": "LPXFujjoSCkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- 1. RE-CREATE TEST SET (Using the fixed 'feature_names') ---\n",
        "# We split the data again to ensure we have a clean Test Set that matches the model's structure\n",
        "# 80% Train, 10% Val, 10% Test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['target'])\n",
        "\n",
        "# Create the dataset using the CORRECT 'feature_names' (Length: 11)\n",
        "test_dataset = ReviewDataset(test_df, tokenizer, MAX_LEN, feature_names)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(f\"Test Set Re-created. Features expected: {len(feature_names)}\")\n",
        "\n",
        "# --- 2. EVALUATION FUNCTION ---\n",
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    real_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            structured_features = batch['structured_features'].to(DEVICE)\n",
        "            targets = batch['targets'].to(DEVICE)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                structured_features=structured_features\n",
        "            )\n",
        "\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            real_values.extend(targets.cpu().tolist())\n",
        "\n",
        "    return predictions, real_values\n",
        "\n",
        "# --- 3. RUN EVALUATION ---\n",
        "print(\"Evaluating on Test Set...\")\n",
        "y_pred, y_test = get_predictions(model, test_loader)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Original', 'Fake (CG)']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Original', 'Fake'], yticklabels=['Original', 'Fake'])\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vyn3qr_RTjFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small list of manual tests\n",
        "manual_tests = [\n",
        "    # 1. Real human review (imperfect grammar, specific details)\n",
        "    \"The box arrived crushed which was annoying but the product itself works fine. I used it for my camping trip.\",\n",
        "\n",
        "    # 2. Obvious AI review (Generic, overly enthusiastic, perfect grammar)\n",
        "    \"I absolutely love this product! It features a robust design and an aesthetically pleasing interface that enhances user experience.\",\n",
        "\n",
        "    # 3. Tricky AI review (Short)\n",
        "    \"Great item. Highly recommended.\"\n",
        "]\n",
        "\n",
        "# Use the wrapper we made for SHAP earlier to predict these\n",
        "probs = f(manual_tests)\n",
        "predictions = np.argmax(probs, axis=1)\n",
        "\n",
        "print(\"Predictions (0=Original, 1=Fake):\", predictions)\n",
        "print(\"Probabilities:\", probs)"
      ],
      "metadata": {
        "id": "WIoDpFfwVaxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT+XGBoost"
      ],
      "metadata": {
        "id": "WEDeEZVvX_Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import xgboost as xgb\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BATCH_SIZE = 32  # Larger batch size is fine for inference\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on {DEVICE}...\")\n",
        "\n",
        "# --- 1. DATA LOADING & PREPROCESSING ---\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('fake reviews dataset.csv')\n",
        "df['target'] = df['label'].apply(lambda x: 1 if x == 'CG' else 0)\n",
        "\n",
        "# Feature Engineering (Structured Data)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['category']),\n",
        "        ('num', StandardScaler(), ['rating'])\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "structured_features = preprocessor.fit_transform(df[['category', 'rating']])\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# FIX: Rename 'rating' to 'scaled_rating' to avoid collision\n",
        "feature_names = [f\"scaled_{name}\" if name == 'rating' else name for name in feature_names]\n",
        "\n",
        "# Combine into a clean DataFrame\n",
        "structured_df = pd.DataFrame(structured_features, columns=feature_names)\n",
        "df_final = pd.concat([df[['text_', 'target']], structured_df], axis=1)\n",
        "\n",
        "# --- 2. BERT FEATURE EXTRACTION ---\n",
        "print(\"Loading BERT model...\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)\n",
        "bert_model.eval() # Set to evaluation mode (crucial!)\n",
        "\n",
        "def extract_bert_features(texts, batch_size=32):\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Process in batches to show progress and avoid memory errors\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting BERT Embeddings\"):\n",
        "        batch_texts = texts[i : i + batch_size]\n",
        "\n",
        "        # Tokenize\n",
        "        encoded_input = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Inference (No Gradients needed = FAST)\n",
        "        with torch.no_grad():\n",
        "            output = bert_model(**encoded_input)\n",
        "            # Take the [CLS] token (first token) from the last hidden layer\n",
        "            cls_embeddings = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            all_embeddings.append(cls_embeddings)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Run extraction (This takes the most time, maybe 5 mins)\n",
        "text_features = extract_bert_features(df_final['text_'].tolist(), BATCH_SIZE)\n",
        "\n",
        "print(f\"Text Features Shape: {text_features.shape}\")\n",
        "print(f\"Structured Features Shape: {structured_features.shape}\")\n",
        "\n",
        "# --- 3. COMBINE FEATURES ---\n",
        "# Concatenate BERT vectors (768 dims) + Structured vectors (11 dims)\n",
        "X_combined = np.hstack((text_features, structured_features))\n",
        "y = df_final['target'].values\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 4. TRAIN XGBOOST ---\n",
        "print(\"Training XGBoost Classifier...\")\n",
        "# We use a robust configuration\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=200,      # Number of trees\n",
        "    learning_rate=0.05,    # Step size\n",
        "    max_depth=6,           # Depth of trees\n",
        "    subsample=0.8,         # Prevent overfitting\n",
        "    colsample_bytree=0.8,  # Prevent overfitting\n",
        "    eval_metric='logloss',\n",
        "    n_jobs=-1              # Use all CPU cores\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# --- 5. EVALUATE ---\n",
        "print(\"\\nEvaluating...\")\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Original', 'Fake']))\n",
        "\n",
        "# --- 6. EXPLAINABILITY (Feature Importance) ---\n",
        "# Create a full list of feature names: 768 BERT dims + 11 Structured names\n",
        "bert_dim_names = [f\"bert_emb_{i}\" for i in range(text_features.shape[1])]\n",
        "all_feature_names = bert_dim_names + feature_names\n",
        "\n",
        "# Map names to the model for plotting\n",
        "xgb_clf.get_booster().feature_names = all_feature_names\n",
        "\n",
        "# Plot top 15 features\n",
        "plt.figure(figsize=(10, 8))\n",
        "xgb.plot_importance(xgb_clf, max_num_features=15, importance_type='weight', height=0.5)\n",
        "plt.title(\"Top 15 Features (BERT Dimensions vs Metadata)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xib7_IghX-f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Previous Model (99%): We were fine-tuning BERT. This means we allowed BERT to rewrite its internal brain to memorize the specific vocabulary quirks of this specific dataset. It likely learned extremely specific patterns (e.g., \"if the word 'superb' appears, it's 100% fake\").\n",
        "\n",
        "This Model (90%): We froze BERT. We forced the model to use a \"general English\" understanding of the text (trained on Wikipedia/Books) without adapting to this specific dataset.\n",
        "\n",
        "The Verdict: A 90% score on a frozen model is arguably a more impressive result for a real-world scenario. It suggests that the fundamental semantics of fake reviews are distinct enough to be caught by general language models + metadata, without needing expensive fine-tuning."
      ],
      "metadata": {
        "id": "pMmXMx5nlWaz"
      }
    }
  ]
}